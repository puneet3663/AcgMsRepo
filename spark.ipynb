{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1733376039799
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**Explode very powerful**</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame with nested data\n",
        "data = [\n",
        "    (1, [\"apple\", \"banana\", \"cherry\"]),\n",
        "    (2, [\"grape\", \"orange\"])\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"fruits\"])\n",
        "\n",
        "# Explode the \"fruits\" column\n",
        "exploded_df = df.withColumn(\"fruit\", explode(\"fruits\"))\n",
        "\n",
        "exploded_df.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "24/12/05 05:41:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---+--------------------+------+\n| id|              fruits| fruit|\n+---+--------------------+------+\n|  1|[apple, banana, c...| apple|\n|  1|[apple, banana, c...|banana|\n|  1|[apple, banana, c...|cherry|\n|  2|     [grape, orange]| grape|\n|  2|     [grape, orange]|orange|\n+---+--------------------+------+\n\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733377302616
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.getOrCreate()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733376054790
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = [(\"Alice\", 1, 1000, \"Address1\"),\n",
        "         (\"Bob\", 2, 2000, \"Address2\"),\n",
        "         (\"Charlie\", 3, 3000, \"Address3\"),\n",
        "         (\"David\", 4, 4000, \"Address4\"),\n",
        "         (\"Eve\", 5, 5000, \"Address5\")]\n",
        "\n",
        "data2 = [(1, 25, \"Married\"),\n",
        "         (2, 30, \"Single\"),\n",
        "         (3, 28, \"Married\"),\n",
        "         (4, 32, \"Single\"),\n",
        "         (5, 27, \"Married\")]"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733220149678
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.createDataFrame(data1, [\"student_name\", \"id\", \"salary\", \"address\"])\n",
        "df2 = spark.createDataFrame(data2, [\"id\", \"age\", \"married_status\"])\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733220151786
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df=df1.join(df2,\"id\")\n",
        "joined_df.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---+------------+------+--------+---+--------------+\n| id|student_name|salary| address|age|married_status|\n+---+------------+------+--------+---+--------------+\n|  1|       Alice|  1000|Address1| 25|       Married|\n|  2|         Bob|  2000|Address2| 30|        Single|\n|  3|     Charlie|  3000|Address3| 28|       Married|\n|  4|       David|  4000|Address4| 32|        Single|\n|  5|         Eve|  5000|Address5| 27|       Married|\n+---+------------+------+--------+---+--------------+\n\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733223735955
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**Broad Case Join**</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data3 = [(1, \"DeptA\"),\n",
        "         (2, \"DeptB\"),\n",
        "         (3, \"DeptC\"),\n",
        "         (4, \"DeptD\"),\n",
        "         (5, \"DeptE\")]\n",
        "\n",
        "df3=spark.createDataFrame(data3, [\"id\",\"department\"])\n",
        "\n",
        "joined_df = df1.join(df2, \"id\").join(df3.hint(\"broadcast\"), \"id\")\n",
        "\n",
        "joined_df.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---+------------+------+--------+---+--------------+----------+\n| id|student_name|salary| address|age|married_status|department|\n+---+------------+------+--------+---+--------------+----------+\n|  1|       Alice|  1000|Address1| 25|       Married|     DeptA|\n|  2|         Bob|  2000|Address2| 30|        Single|     DeptB|\n|  3|     Charlie|  3000|Address3| 28|       Married|     DeptC|\n|  4|       David|  4000|Address4| 32|        Single|     DeptD|\n|  5|         Eve|  5000|Address5| 27|       Married|     DeptE|\n+---+------------+------+--------+---+--------------+----------+\n\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733223745786
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**Distinct Function **</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinctDF = joined_df.distinct()\n",
        "print(\"Distinct count: \"+str(distinctDF.count()))\n",
        "distinctDF.show(truncate=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Distinct count: 5\n+---+------------+------+--------+---+--------------+----------+\n|id |student_name|salary|address |age|married_status|department|\n+---+------------+------+--------+---+--------------+----------+\n|1  |Alice       |1000  |Address1|25 |Married       |DeptA     |\n|2  |Bob         |2000  |Address2|30 |Single        |DeptB     |\n|3  |Charlie     |3000  |Address3|28 |Married       |DeptC     |\n|4  |David       |4000  |Address4|32 |Single        |DeptD     |\n|5  |Eve         |5000  |Address5|27 |Married       |DeptE     |\n+---+------------+------+--------+---+--------------+----------+\n\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733223776205
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**dropDuplicates**</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = joined_df.dropDuplicates()\n",
        "print(\"Distinct count: \"+str(df2.count()))\n",
        "df2.show(truncate=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Distinct count: 5\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r[Stage 57:>                                                         (0 + 2) / 2]\r\r                                                                                \r"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733223806626
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**dropDuplicates_2_Columns**</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropDisDF = joined_df.dropDuplicates([\"department\",\"salary\"])\n",
        "print(\"Distinct count of department & salary : \"+str(dropDisDF.count()))\n",
        "dropDisDF.show(truncate=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Distinct count of department & salary : 5\n+---+------------+------+--------+---+--------------+----------+\n|id |student_name|salary|address |age|married_status|department|\n+---+------------+------+--------+---+--------------+----------+\n|1  |Alice       |1000  |Address1|25 |Married       |DeptA     |\n|2  |Bob         |2000  |Address2|30 |Single        |DeptB     |\n|3  |Charlie     |3000  |Address3|28 |Married       |DeptC     |\n|4  |David       |4000  |Address4|32 |Single        |DeptD     |\n|5  |Eve         |5000  |Address5|27 |Married       |DeptE     |\n+---+------------+------+--------+---+--------------+----------+\n\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733223835689
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**repartition**</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = joined_df.repartition(6)\n",
        "print(df2.rdd.getNumPartitions())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "6\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733223867947
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**UDF Example**</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def convertCase(str):\n",
        "    resStr=\"\"\n",
        "    arr = str.split(\" \")\n",
        "    for x in arr:\n",
        "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
        "    return resStr \n",
        "\n",
        "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
        "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
        "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
        "     .show(truncate=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+-----+------------+\n|Seqno|Name        |\n+-----+------------+\n|1    |john jones  |\n|2    |tracey smith|\n|3    |amy sanders |\n+-----+------------+\n\n+-----+-------------+\n|Seqno|Name         |\n+-----+-------------+\n|1    |John Jones   |\n|2    |Tracey Smith |\n|3    |Amy Sanders  |\n+-----+-------------+\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r[Stage 94:>                                                         (0 + 1) / 1]\r"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733224290234
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**Complex data type**</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    {\"id\": 1, \"items\": [\"item1\", \"item2\", \"item3\"]},\n",
        "    {\"id\": 2, \"items\": [\"item4\", \"item5\"]}\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---+--------------------+\n| id|               items|\n+---+--------------------+\n|  1|[item1, item2, it...|\n|  2|      [item4, item5]|\n+---+--------------------+\n\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733377504026
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**Using python and explode**</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, udf, size, collect_set, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"NestedDataFrame\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame with nested data\n",
        "data = [\n",
        "    {\"id\": 1, \"items\": [\"item1\", \"item2\", \"item3\",\"item56\",\"item546\"]},\n",
        "    {\"id\": 2, \"items\": [\"item4\", \"item5\"]}\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"items\", ArrayType(StringType()), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Calculate total items before exploding\n",
        "df = df.withColumn(\"total_items\", size(\"items\"))\n",
        "\n",
        "# Define a custom function to extract a feature (adjust the return type as needed)\n",
        "def extract_feature(item):\n",
        "    feature_value = \"hello\"\n",
        "    return feature_value  # Assuming feature_value is a string\n",
        "\n",
        "\n",
        "# Explode the `items` array and apply the custom function\n",
        "exploded_df = df.withColumn(\"item\", explode(\"items\")) \\\n",
        "               .withColumn(\"feature\", udf(extract_feature, StringType())(\"item\"))\n",
        "\n",
        "# Group by `id` and calculate aggregate metrics\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, udf, size, collect_set, col, first\n",
        "\n",
        "\n",
        "result_df = exploded_df.groupBy(\"id\") \\\n",
        "               .agg(\n",
        "                   first(\"total_items\").alias(\"total_items\"),\n",
        "                   collect_set(\"feature\").alias(\"unique_features\")\n",
        "               )\n",
        "\n",
        "df.show()\n",
        "result_df.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "24/12/05 05:45:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---+--------------------+-----------+\n| id|               items|total_items|\n+---+--------------------+-----------+\n|  1|[item1, item2, it...|          5|\n|  2|      [item4, item5]|          2|\n+---+--------------------+-----------+\n\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733377516886
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <mark>_<u>**Using SQL and explode**</u>_</mark>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, udf, size, collect_set, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
        "\n",
        "data = [\n",
        "    {\"id\": 1, \"items\": [\"item1\", \"item2\", \"item3\",\"item56\",\"item546\"]},\n",
        "    {\"id\": 2, \"items\": [\"item4\", \"item5\"]}\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"items\", ArrayType(StringType()), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Register the original DataFrame with a temporary view\n",
        "df.createOrReplaceTempView(\"original_data\")\n",
        "\n",
        "df.show()\n",
        "\n",
        "# Register the exploded DataFrame with a temporary view\n",
        "exploded_df = df.withColumn(\"total_items\", size(\"items\")) \\\n",
        "               .withColumn(\"item\", explode(\"items\")) \\\n",
        "               #.withColumn(\"feature\", udf(extract_feature, StringType())(\"item\"))\n",
        "exploded_df.createOrReplaceTempView(\"exploded_data\")\n",
        "exploded_df.show()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---+--------------------+-----------+\n| id|               items|total_items|\n+---+--------------------+-----------+\n|  1|[item1, item2, it...|          5|\n|  2|      [item4, item5]|          2|\n+---+--------------------+-----------+\n\n+---+--------------------+-----------+-------+\n| id|               items|total_items|   item|\n+---+--------------------+-----------+-------+\n|  1|[item1, item2, it...|          5|  item1|\n|  1|[item1, item2, it...|          5|  item2|\n|  1|[item1, item2, it...|          5|  item3|\n|  1|[item1, item2, it...|          5| item56|\n|  1|[item1, item2, it...|          5|item546|\n|  2|      [item4, item5]|          2|  item4|\n|  2|      [item4, item5]|          2|  item5|\n+---+--------------------+-----------+-------+\n\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733377523912
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL query to calculate total items and unique features grouped by id\n",
        "sql_query = \"\"\"\n",
        "SELECT id,\n",
        "       first(total_items) AS total_items,\n",
        "       collect_set(feature) AS unique_features\n",
        "FROM exploded_data\n",
        "GROUP BY id\n",
        "\"\"\"\n",
        "\n",
        "# Execute the SQL query on the temporary view \"exploded_data\"\n",
        "result_df_1 = spark.sql(sql_query)\n",
        "\n",
        "# Display the results (optional)\n",
        "result_df_1.show()\n",
        "\n",
        "spark.sql(\"select distinct id, items, total_items from exploded_data\").show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733377466524
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
